---
layout:     post
title:      循环神经网络
subtitle:   "CNN"
date:       2020-10-26
author:     "邬小达"
header-img: "img/post-programming-language.jpg"
tags:
    - 人工智能
---

循环神经网络相比前馈神经网络，其不同之处在于，网络中每层的神经元不但可以接收其它层的信息，还可以接收自身的信息。这就使得该网络具有记忆性，可以用来处理时序数据，如视频、语音、文本、点击等。

其中，t时刻与t-1时刻的关系可以表示为：
$$
h_t = f({w_t}{h_{(t-1)}} + b_t)
$$

t时刻的信息不止取决于上一个时刻的信息，还取决于过去一段时间的信息。


扩展：LSTM、递归神经网络、图网络

应用场景：机器翻译、序列标注、图像描述、推荐系统、智能聊天机器人、自动作词作曲等

* 序列到类别模式
* 同步的序列到序列模式
* 异步的序列到序列模式

### 简单循环网络

在简单循环网络中，隐藏层的状态不仅与上一时刻的状态相关，还与当前时刻的输入有关。这使得当前状态可以包含过去所有状态的信息。

比如，针对文本数据，循环神经网络很好的模拟了人阅读一篇文章的顺序。从前到后阅读文章中的每一个文字，并且将前面阅读到的有用信息与当前的文字相关联，使得网络具有一定的记忆能力。
$$
h_{t} = f(Uh_{(t-1)} + Wx_t + b_{t})\\
y = g(Vh_t + C)
$$

与前馈神经网络可以拟合任何连续函数类似，完全连接的循环网络也可以拟合任何非线性动力系统。

> 动力系统：系统状态按照一定的规律随时间变化的系统。如钟摆运动、台球轨迹等。

### 长程依赖问题

梯度消失或爆炸问题，是深度学习的一个基本问题。在利用反向传播算法求参数时，误差从输出层反向传播，每一层都要乘以该层的激活函数的导数。比如Sigmoid函数，在输入值很大或者很小时，其偏导都会接近于0，具有饱和性。当输入达到一定值的情况下，输出就不会发生明显变化了。当网络很深时，梯度会因为反向传播而不断衰减并几乎趋于0。因此，无法对前层的参数进行有效的学习。

对于梯度爆炸问题，可以采用权重衰减或梯度截断的方法（梯度截断指的是当梯度的范式大于某个给定值时，对梯度进等比放缩）。而梯度消失问题，则相对棘手。

对于循环神经网络而言，相邻时刻之间是联系在一起的，这使得它们的偏导同时都小于1，或者大于1。如果输入的序列很长时，当前时刻与历史时刻的间隔（t-k）就会很大，经过多阶段传播后的梯度会更容易消失（大部分情况）或爆炸（很少，但对优化过程影响很大）。因此，这使得循环神经网络很难构建长时间间隔状态之间的依赖关系。这就是循环神经网络所特有的长程依赖问题。

解决长程依赖问题的方法是引入门控机制（Gating Mechanism），使神经元可以选择性地加入新信息，或者选择性地遗忘过去积累的信息。

> 小技巧：当采用ReLU作为循环神经网络中隐藏层的激活函数，并将W的取值限定在单位矩阵附近时，可以使效果接近于长短期记忆网络。

### 长短期记忆网络

长短期记忆网络（Long Short-Term Memory Network，LSTM）引入了门控机制，有效地解决了长程依赖问题。
$$
c_t = f_t \bigodot c_{t-1} + i_t \bigodot \tilde{c_t}
$$

$$
h_t = o_t \bigodot tanh(c_t)
$$

$$
\tilde{c_t} = tanh({W_t}{x_t} + {U_c}{h_{t-1}} + b_c)
$$

$$
其中遗忘门f_t、输入门i_t、输出门o_t是用来控制信息传递路径的，这三个门的取值均在[0,1]之间，表示为Sigmoid函数：
$$

$$
f_t = \sigma({W_f}{x_t} + {U_f}{h_{t-1}} + b_f)
$$

$$
i_t = \sigma({W_i}{x_t} + {U_i}{h_{t-1}} + b_i)
$$

$$
o_t = \sigma({W_o}{x_t} + {U_o}{h_{t-1}} + b_o)
$$

* 遗忘门控制上一个时刻的内部状态：需要遗忘多少信息
* 输入门控制当前时刻的候选状态：需要保存多少信息
* 输出门控制当前时刻的内部状态：需要将多少信息输出给下一层

现代的LSTM通常使用sigmoid和Tanh作为激活函数。因为这两个函数都是饱和的激活函数。

LSTM已经成功应用于很多场景，如手写识别、语音识别、机器翻译、手写生成等。

### 循环神经网络的输入和输出

常见的循环神经网络是同步的序列到序列模式，即输入序列和输出序列的长度相同。

![循环神经网络同步的序列到序列模式](https://i.loli.net/2020/10/19/2XKHx1IRGs3ibpj.png)

#### 1.文本数据

我们以一系列的文本作为循环神经网络的输入，来构建一个语言模型。

输入：一段长度为T的文本序列
$$
"天",“地”,“悠”,“悠”,“过”,”客“,“匆”,匆“,”潮“,”起“,"又","潮"
$$

预测目标：向前移动1个时间长度
$$
“地”,“悠”,“悠”,“过”,”客“,“匆”,匆“,”潮“,”起“,"又","潮","落”
$$

### 2.金融数据

我们想用T时刻及以前的沪深300价格来预测其T+5天后的价格

输入：一段长度为T的沪深300序列

$$
p_0,p_1,..,p_t,..,p_{t+n}
$$

预测目标：向前移动5个时间长度

$$
p_5,p_6,..,p_{t+5},..,p_{t+n+5}
$$

### 小技巧

1. 长序列处理

对于那些非常长，以至于RNN无法处理的序列，还可以使用卷积层和循环层叠加的方式。

首先，用一维卷积神经网络将长序列转换为高级特征组成的更短序列（下采样）。然后，对更短的序列，使用循环神经网络处理。——《Python深度学习》P193

2. 构建高性能的深度卷积神经网络

使用残差连接、批标准化、深度可分离卷积。其中深度可分离卷积在未来很可能会替代卷积层。因为它使得模型更加简洁（参数更少）、速度更快（更少的浮点运算）、还提高了性能（几个百分点）。

