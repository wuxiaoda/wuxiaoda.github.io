---
layout:     post
title:      "深度学习中的优化算法"
subtitle:   "Optimization_Algorithms_DL"
date:       2020-10-27
author:     "邬小达"
header-img: "img/post-bg-js-version.jpg"
tags:
    - 人工智能
---

深度学习模型的优化算法体现在网络优化和正则化。

网络优化。首先深度学习的损失函数是非凸函数，很难找到全局最优解。其次，深度学习有多个隐藏层和大量的神经元，这使得参数非常多。因此，需要设置合理的算法，提高计算效率。此外，深度学习还存在梯度消失和梯度爆炸的问题。

正则化。由于通用近似定理，深度学习可以拟合任意一个给定的连续函数。但这也导致其在训练数据上容易过拟合。因此，需要进行正则化减少泛化误差而非训练误差。正则化的方法是使参数变小甚至趋于0。

| 类别     | 名称                                           | 优化算法                                         |
| -------- | ---------------------------------------------- | ------------------------------------------------ |
| 网络优化 | 改进梯度下降法                                 | 随机/批量/小批量梯度下降                         |
| 网络优化 | 学习率调整                                     | 学习率衰减                                       |
| 网络优化 | 参数初始化                                     | 预训练初始化、随机初始化、固定值初始化           |
| 网络优化 | 数据预处理                                     | 标准化                                           |
| 网络优化 | 逐层归一化                                     | 批量归一化、层归一化、权重归一化、局部响应归一化 |
| 网络优化 | 超参数优化                                     | 随机搜索、网格搜索、贝叶斯优化、动态资源分配     |
| 网络优化 | 对随机梯度下降法的修正（防止落入极小值和鞍点） | Momentum方法、AdaGrad方法、Adam方法              |
| 正则化   | L1和L2正则化                                   | 在深度神经网络上的效果不佳                       |
| 正则化   | 权重衰减（梯度更新时）                         | 在标准的随机梯度下降中，效果与L2相同             |
| 正则化   | 提前停止                                       | 若验证集上的错误率不再下降，则停止迭代           |
| 正则化   | Dropout                                        | 随机丢弃一部分神经元                             |
| 正则化   | 数据增强                                       | 增加数据量：旋转、缩放、平移、加噪声             |
| 正则化   | 标签平滑                                       | 在标签中添加噪声                                 |

> 贝叶斯优化：是根据当前已经试验过的参数组合，来预测下一个可能带来最大收益的组合。

经验之谈

* Dropout是模型集成方法中最高效与常用的技巧
* 深度神经网络的重要参数有：学习率、权重衰减系数、Dropout比例
* 批量归一化可以有效规避复杂参数对网络训练的影响，在加速训练的同时提高模型的泛化能力
* 关于对梯度下降法的优化，很多研究人员和技术人员喜欢用Adam，Momemtum和AdaGrad也是值得一试的方法。、

## Dropout

在训练一个深度神经网络时，我们可以随机丢弃一部分神经元来避免过拟合。比如，设置一个固定的概率p，对每一个神经元都以概率p来判定要不要保留。

对于每份的小批量数据，Dropout通过随机丢弃一部分神经元，使得每次生成的神经网络都不太相同。并且在做预测时，对神经元的输出乘以删除的概率，可以取得模型的平均值。Dropout的这种做法类似于机器学习中的bagging方法。

> bagging方法：使用多个模型进行单独训练；在预测时，则取每个模型的平均值作为最终预测结果。

应用Dropout后，在训练阶段，第L层与第L-1层的关系可以表示为：
$$
r \backsim Bernoulli(p)
$$

$$
a^{(L)} = f_L(w^{L}r^{(L-1)}a^{(L-1)} + b^{L})
$$

其中，Bernoulli(p)的含义是以概率p随机生成一个取值为0或1的向量。

在训练时，激活神经元的平均数量为原来的p倍，而在测试时，所有神经元都是被激活的。因此，在测试阶段每个神经元的参数都需要乘以概率系数p，使得训练和测试时的网络输出一致。

> 为什么在测试阶段要乘以概率系数p？因为在训练时，是对每一层的某一些输出值做了舍弃（归零）。而使用优化算法得到的参数是在输出值之前做的操作，但是同样会应用于测试阶段。这就导致，测试阶段的输出值并没有被舍弃。因此，需要在测试阶段将参数乘以概率系数p。

Dropout使得神经网络不会给予任何一个输入太大的权重。反过来，神经网络对于特征不敏感，有助于提升模型的泛化能力。从结果上来看，Dropout将产生与L2正则化相同的收缩权重的效果。

以下是使用Dropout的一般原则：

* 通常丢弃率控制在20%-50%，从20%开始尝试
* Dropout适合在深度的神经网络中应用
* 对于神经元较少的层，丢弃率(keep_prob)接近于1；对于神经元较多的层，丢弃率（keep_prob）设置为0.5或更小
* 把学习速率扩大10~100倍，冲量值调高到0.9~0.99
* 限制网络模型的权重。大的学习速率往往会导致大的权重值。对网络的权重值做最大范数的正则化，被证明能提升模型性能。

### 权重的初始值

在对权重的初始值进行设置时，不能将其全部设为相同的值。这会导致参数无法学习。假如第1层和第2层的权重相同，那么在输入值进行正向传播时，第2层的神经网络都变为了相同的值。然后在误差反向传播时，第2层的所有的权重值都会进行相同的更新。这导致权重都被更新为相同的值。

Xavier Glorot的论文中推荐，像sigmoid函数和tanh函数这类线性函数的权重初始值可以设置为，Xavier初始值：
$$
若前一层的节点数为n，则初始值使用标准差为\frac{1}{\sqrt n}的高斯分布
$$

Kaiming He等人，推荐ReLU函数的权重初始值为，He初始值：
$$
若前一层的节点数为n, 则初始值使用标准差为\frac{2}{\sqrt n}的高斯分布
$$

### 批量归一化

批量归一化为为了解决非常深的神经网络中，层之间的相互影响。因为每层的参数变化都会导致下一层的输入发生变化，容易增加训练的复杂度和过拟合的风险。

批量归一化发生在仿射变换/卷积计算之后，激活函数之前。具体操作是针对每一批数据，在网络的每一层输入之前，对单个神经元进行归一化处理
$$
\hat z = \frac{z^L - E(z^L)}{\sqrt{ var(z^L) + \epsilon}}
$$

$$
z^L = w^{L}a^{(L-1)} + b^{L}
$$

$$
在这里，不对a^{L-1}做标准化，是因为效果不如对z^L做标准化
$$

然后，由于标准化后的数据，破坏了之前学习到的特征分布，因此需要加入两个学习参数来恢复原始数据的分布
$$
a^{(L)} = f(\gamma \hat z^{L} + \beta)
$$

$$
加入这两个参数的好处在于，如果归一化是无用的，那么只要学出\gamma=\sqrt{ var(z^L) + \epsilon},\beta=E(z^L)，就可以不做归一化。
$$

逐层归一化的好处在于不仅可以提高了优化效率，而且也能提高模型的泛化能力。批量归一化对卷积网络和具有sigmoid非线性函数的网络有显著的影响。

注意：和Dropout一样，批量归一化在训练和预测阶段下的计算结果也是不一样的。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。**？疑问：对谁移动平均**

批量归一化的优点

* 可以使学习快速进行（可以增大学习率）
* 不那么依赖于初始值（对于初始值不敏感）
* 抑制过拟合（降低Dropout等的必要性）

### 学习率

学习率是网络优化中非常重要的参数。针对学习率也开发出了很多优化算法。

* 学习率衰减
* 学习率预热
* 周期性学习率调整
* 自适应调整学习率：AdaGrad、RMSprop、AdaDelta

学习率衰减会根据迭代次数的增加，依次减小学习率的值。如按照分段常数衰减、指数衰减、余弦衰减。

学习率预热指的是在最初几轮迭代时，采用比较小的学习率，等梯度下降到一定程度后再恢复初始的学习率。

周期性学习率调整会在训练过程中周期性地增大学习率。其目的是让参数可以逃离鞍点或者尖锐最小值。

### 权重衰减

权重衰减在每次参数更新时，引入了一个衰减系数

$$
\theta_t = (1- \beta){\theta_{t-1}} - \alpha g_t
$$

$$
其中\beta为权重衰减系数，\alpha为学习率，g_t为第t步的梯度
$$

在标准的随机梯度下降中，权重衰减的做法与L2正则化的效果相同。但在一些较为复杂的优化方法（如Adam）中，两者并不等价。

### 超参数的优化方法

随机搜索、贝叶斯优化、遗传算法、网格搜索。

通常情况下，随机搜索是最好的解决方案。Python中有一个用于超参数优化的库Hyperopt，很有用。Hyperopt寻找最优参数的方法是通过Parzen估计器的树来预测哪组超参数可能得到更好的结果。

### 超参数对模型的影响

超参数的调参技巧：逐渐缩小超参数的“好值”的范围

* 设定参数的范围。以10的阶乘为尺度，如0.001，0.01，0.1等
* 对参数随机采样
* 通过训练数据训练模型，验证数据评估效果。（把每次学习的epoch调小，可以加快调参的速度）
* 逐渐缩小超参数范围

| 超参数       | 容量何时增加 | 原因                                                         | 注意事项                                                     |
| ------------ | ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 隐藏单元数量 | 增加         | 增加隐藏单元数量会增加模型的表示能力                         | 几乎模型每个操作所需的时间和内存代价都会随隐藏单元数量的增加而增加 |
| 学习率       | 调至最优     | 不正确的学习率，不管是太高还是太低都会由于优化失败而导致低有效容量的模型 |                                                              |
| 卷积核宽度   | 增加         | 增加卷积核宽度会增加模型的参数数量                           |                                                              |
| 隐式零填充   | 增加         | 在卷积之前隐式添加零能保持较大尺寸的表示                     | 大多数操作的时间和内存代价会增加                             |
| 权重衰减系数 | 降低         | 降低权重衰减系数使得模型参数可以自由地变大                   |                                                              |
| Dropout比率  | 降低         | 较少地丢弃单元可以更多地让单元彼此“协力”来适应训练集         |                                                              |

