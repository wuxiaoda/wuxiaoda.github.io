---
layout:     post
title:      "机器学习三要素"
subtitle:   "Three_Elements_of_ML"
date:       2020-10-11
author:     "邬小达"
header-img: "img/post-bg-js-version.jpg"
tags:
    - 人工智能
---

### 模型

模型：机器学习希望找到一个模型，可以近似表示真实的数据。以逻辑回归为例：
$$
log(\frac{p(y=1|x)}{1-p(y=1|x)}) = wx + b
$$

### 学习法则

学习法则：一个好模型的预测值与真实值之间的差异应该尽可能小。在机器学习中，损失函数被用来衡量这种差异。常见的损失函数有平方损失函数和交叉熵损失函数。

平方损失函数用于回归问题，定义为：

$$
L(y, f(x;\theta)) = \frac{1}{2}*(y-f(x;\theta))^2
$$

交叉熵损失函数用于分类问题，反应的是两个概率分布的距离（不是欧式距离）。定义为：
$$
L(y, f(x;\theta)) = -\sum_{c=1}^Cy_clogf_c(x;\theta)
$$

$$
y_c是真实标签
$$

$$
f_c(x;\theta)是模型的输出
$$

因为在多分类问题中，一个样本的标签如果属于C类，那么其y值在C类上表示为1，在其它类别上都表示为0。因此可以简化为：
$$
L(y, f(x;\theta)) = -log(f_c(x;\theta))
$$

$$
y是属于类别C
$$

对于N个样本求和的损失函数可以表示为：
$$
L(y, f(x;\theta)) = -\frac{1}{N}*\sum_N{log(f_k(x;\theta))}
$$

以逻辑回归为例，其损失函数可以表示为：
$$
L(y, f(x;\theta)) = -\frac{1}{N}*\sum_N{[y_ilog(p_i) + (1-y_i)log(1-p_i)]}
$$

一个好的模型要满足上述的损失函数最小化，即经验风险最小化原则。但由于模型只是在已有的数据上做到了很好地拟合，这会导致在未知数据上的错误率高。这种现象被称为过拟合。为了解决这一问题，会在损失函数最小化的基础上，引入参数的正则化（使得参数趋于0），来控制模型的复杂程度。这种原则被称为结构风险最小化。因此，损失函数要综合考虑经验风险最小化和结构风险最小化。换句话说，我们既要考虑模型的拟合能力，又要控制模型的复杂程度。其中，对模型进行偏差-方法分解，可以很好地表示拟合能力和复杂度。

* 偏差。指的是一个模型在不同训练集上的平均性能，可以用来衡量一个模型的拟合能力。
* 方差。指的是一个模型在不同训练集上的差异，可以用来衡量一个模型是否容易过拟合。

### 优化算法

确定了训练集、模型、学习法则后，要找到最优的模型，这就变成了一个优化问题，既寻找参数和超参数。优化算法寻找的是模型的参数。具体来说，是针对训练数据计算损失函数的值，然后找出使该值尽可能小的参数。

常见的是使用梯度下降法。梯度下降类似于如何寻找一个盆地最低点的过程。

对于某个未知的参数而言，我们对参数的损失函数求偏导时，导数表示的是：改变参数的值时，损失函数的值会如何变化。当导数为负时，只要将参数往正方向改变，就能减小损失函数的值；同理，当导数为正时，将参数往负方向改变，也会减小损失函数的值。

>根据“梯度”的性质可以发现：如果实值函数F(x)在点x0处可微且有定义，那么函数F(x)在点x0处沿着梯度相反的方向下降最快。

因此，在寻找损失函数的局部最小值时，会通过求损失函数求偏导来得到某个参数的梯度。然后设定学习率和初始参数，沿着梯度的反方向下降，并用真实数据迭代此过程，得出该参数的估计值。

$$
{\theta}_{t} = {\theta}_{(t-1)} - {\alpha} \frac{\partial f}{\partial {\theta_{(t-1)}}}
$$

注意

* 从导数的角度来看，要使参数能够正常更新，导数不应为0
* 梯度寻找的是梯度为0的地方，但是那个地方不一定是最小值。它也可能是局部极小值和鞍点。

以逻辑回归为例，对Sigmoid函数求一阶偏导：
$$
\frac{\partial {\hat y}}{\partial x} = \hat y(1-\hat{y})\\
\frac{\partial {\hat y}}{\partial w} = \frac{\partial {\hat y}}{\partial x} \frac{\partial {\hat w}}{\partial w} = \hat y(1-\hat{y})x
$$

参数在交叉熵损失函数上的偏导为：
$$
\frac{\partial {R(w)}}{\partial w} =\frac{1}{N}\sum_{n=1}^Nx(\hat{y}- y)
$$

采用梯度下降法，逻辑回归的训练过程为：

$$
初始化w_0=0,然后迭代更新参数：
$$

$$
w_{t+1} \leftarrow {w}_t - {\alpha} \frac{1}{N}\sum_{n=1}^N{x({\hat y}_{w_t}-y)}
$$

$$
其中\hat{y}_{w_t}表示当参数为w_t时，逻辑回归模型的输出
$$

